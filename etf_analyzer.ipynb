{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Application for an ETF Analyzer\n",
    "\n",
    "In this Project, we build a financial database and web application using SQL, Python, and the Voilà library to analyze the performance of a hypothetical FinTech ETF. This notebook is used for the analysis of a fintech ETF that consists of four stocks: GDOT, GS, PYPL, and SQ. Each stock has its own table in the `etf.db` database. \n",
    "(GDOT: Green Dot Corporation, GS: Goldman Sacks, PYPL: Paypal Holdings, SQ: Block)\n",
    "\n",
    "We analyze the daily returns of the ETF stocks both individually and as a whole. Then deploy the visualizations to a web application by using the Voilà library. The detailed instructions are divided into the following parts:\n",
    "\n",
    "* Analyze a single asset in the ETF\n",
    "\n",
    "* Optimize data access with Advanced SQL queries\n",
    "\n",
    "* Analyze the ETF portfolio\n",
    "\n",
    "* Create a new database where to store the data of the ETF returns equally weighted.The idea is to not to alter the original.\n",
    "\n",
    "* Deploy the notebook as a web application\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports of the required libraries, initiation of the SQLite database, population of the database with records from the `etf.db` seed file that is included in the repository, creates the database engine, and confirms that data tables that it now contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries and dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hvplot.pandas\n",
    "import sqlalchemy\n",
    "import datetime\n",
    "from datetime import date, datetime\n",
    "from sqlalchemy import inspect\n",
    "\n",
    "# Create a temporary SQLite database and populate the database with content from the etf.db seed file\n",
    "database_connection_string = 'sqlite:///etf.db'\n",
    "\n",
    "# Create an engine to interact with the SQLite database\n",
    "engine = sqlalchemy.create_engine(database_connection_string)\n",
    "\n",
    "# Confirm that table names contained in the SQLite database.\n",
    "print(\"Tables names for data from stocks Green Dot Inc, Goldman Scahs Group Inc, Paypal Inc, and Square Inc.\")\n",
    "inspect(engine).get_table_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sqlalchemy.create_engine?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze a single asset in the FinTech ETF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Write a SQL `SELECT` statement by using an f-string that reads all the PYPL data from the database. Using the SQL `SELECT` statement, we execute a query that reads the PYPL data from the database into a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a SQL query to SELECT all of the data from the PYPL table\n",
    "query = \"\"\"\n",
    "SELECT * from PYPL\n",
    "\"\"\"\n",
    "\n",
    "# Use the query to read the PYPL data into a Pandas DataFrame and set index to \"time\"\n",
    "fmt='%Y%m%d %H:%M:%S'\n",
    "pypl_dataframe = pd.read_sql_query(query, con=engine, parse_dates = {'time':fmt} )\n",
    "pypl_dataframe = pypl_dataframe.set_index('time')\n",
    "\n",
    "print(\"\\033[1m  Table with Paypal Inc. prices, volume and daily returns\")\n",
    "pypl_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Use the `head` and `tail` functions to review the first five and the last five rows of the DataFrame. We save the beginning and end dates that are available from this dataset, since we’ll use this information to complete the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Beggining and End date of the Period.\n",
    "print(f\"Beggining of period: {pypl_dataframe.index[0]}\")\n",
    "print(f\"End of period      : {pypl_dataframe.index[-1]}\")\n",
    "\n",
    "#Calculate lenght of period to calculate actual annualized return later\n",
    "period = (pypl_dataframe.index[-1] - pypl_dataframe.index[0])\n",
    "period_in_years = period.days/365.25\n",
    "print(f\"Period in days is: {period.days}, which are {period_in_years:,.2f} years \\n\\n\")\n",
    "\n",
    "# View the first 5 rows of the DataFrame.\n",
    "print(\"\\033[1m  Firsts and lasts columns of the pypl_dataframe, with data of Paypal stock. \\n\")\n",
    "display(pypl_dataframe.head())\n",
    "display(pypl_dataframe.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pypl_dataframe[\"Daily Returns %\"] = pypl_dataframe['daily_returns']*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Interactive visualization for the PYPL daily returns using hvPlot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an interactive visualization with hvplot to plot the daily returns for PYPL.\n",
    "pypl_dataframe.hvplot(\n",
    "    title=\"PYPL Daily Returns (%)\"\n",
    "    ,y='Daily Returns %'\n",
    "    ,xlabel= 'Date'\n",
    "    ,ylabel='Returns (%)'\n",
    "    ,width=800\n",
    ").opts(\n",
    "    color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Interactive visualization for the PYPL cumulative returns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Representing the cummulative investment\n",
    "growth_of_1usd_investment=(1+pypl_dataframe[\"daily_returns\"]).cumprod()\n",
    "\n",
    "#Transforming a series to a dataframe and renaming columns\n",
    "growth_of_1usd_investment = growth_of_1usd_investment.to_frame().rename(columns={'daily_returns':'Growth of 1 USD Investment'})\n",
    "print(\"\\033[1m Table: Evolution of a $1 initial investment on Dec 15th 2016 on the ETF.\")\n",
    "\n",
    "display(growth_of_1usd_investment)\n",
    "\n",
    "# Create an interactive visaulization with hvplot to plot the cumulative returns for PYPL.\n",
    "growth_of_1usd_investment.hvplot(\n",
    "    title=\"Paypal Holdings Inc -- Growth of 1 USD Initial Investment -- Period Dec-16-2016 to Dec 4th 2020\"\n",
    "    ,ylabel=\"Initial Investment \\n plus Cumulative Return\"\n",
    "    ,xlabel= \"Date\"\n",
    "    ,width=900\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize the SQL Queries\n",
    "\n",
    "For this part, we continue to analyze a single asset (PYPL) from the ETF. We use SQL queries to optimize the efficiency of accessing data from the database.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Access the closing prices for PYPL that are greater than 200 by completing the following steps:\n",
    "\n",
    "    - Select the dates where the PYPL closing price was higher than 200.0.\n",
    "\n",
    "    - Read the data from the database into a Pandas DataFrame, and then review the resulting DataFrame.\n",
    "\n",
    "    - Select the “time” and “close” columns for those dates where the closing price was higher than 200.0.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Write a SQL SELECT statement to select the time column \n",
    "# where the PYPL closing price was higher than 200.0.\n",
    "query = \"\"\"\n",
    "SELECT time \n",
    "FROM PYPL\n",
    "WHERE close > 200\n",
    "\"\"\"\n",
    "\n",
    "# Using the query, read the data from the database into a Pandas DataFrame, and convert date strings to date\n",
    "fmt = '%Y%m%d %H:%M:%S'\n",
    "pypl_dates_higher_than_200 = pd.read_sql_query(query, engine, parse_dates={'time':fmt})\n",
    "\n",
    "# Review the resulting DataFrame\n",
    "print(\"\\033[1m Older dates when close price of Paypal is higher than $200. Data comes from SQL database:\")\n",
    "display(pypl_dates_higher_than_200.head())\n",
    "\n",
    "# Select those dates from the pypl dataset\n",
    "pypl_higher_than_200 = pypl_dataframe.loc[pypl_dates_higher_than_200['time'],'close'].to_frame()\n",
    "print(\"\\033[1m  Older dates and close price of Paypal when higher than $200 in pandas dataframe\")\n",
    "display(pypl_higher_than_200.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Find the top 10 daily returns for PYPL by completing the following steps:\n",
    "\n",
    "    -  Write a SQL statement to find the top 10 PYPL daily returns. For this purpose we:\n",
    "\n",
    "        * Select only the “time” and “daily_returns” columns.\n",
    "\n",
    "        * Use `ORDER` to sort the results in descending order by the “daily_returns” column.\n",
    "\n",
    "        * Use `LIMIT` to limit the results to the top 10 daily return values.\n",
    "\n",
    "    - Using the SQL statement, read the data from the database into a Pandas DataFrame, and then review the resulting DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a SQL SELECT statement to select the time and daily_returns columns\n",
    "# Sort the results in descending order and return only the top 10 return values\n",
    "query = \"\"\"\n",
    "SELECT time, daily_returns\n",
    "FROM PYPL\n",
    "ORDER by daily_returns  desc\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "# Using the query, read the data from the database into a Pandas DataFrame\n",
    "# Counting is useful to visualize amount of data, so index is not change to time\n",
    "fmt = '%Y%m%d %H:%M:%S'\n",
    "pypl_top_10_returns = pd.read_sql_query(query, engine, parse_dates={'time':fmt})\n",
    "\n",
    "pypl_top_10_returns['daily_returns']=pypl_top_10_returns['daily_returns']*100\n",
    "\n",
    "print(\"\\033[1m Table with the dates when the top 10 larger daily returns of Paypal Inc. occured:\")\n",
    "display(pypl_top_10_returns[['time']])\n",
    "    \n",
    "# Review the resulting DataFrame\n",
    "print (\"\\n\")\n",
    "print(\"\\033[1m Table with the top 10 larger daily returns of Paypal stock in percentages (%):\")\n",
    "display(round(pypl_top_10_returns,2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the Fintech ETF Portfolio\n",
    "\n",
    "For this part, we build the entire ETF portfolio and then evaluate its performance. To do so, we build the ETF portfolio by using SQL joins to combine all the data for each asset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Write a SQL query to join each table in the portfolio into a single DataFrame. To do so, we complete the following steps:\n",
    "\n",
    "    - Use a SQL inner join to join each table on the “time” column. Access the “time” column in the `GDOT` table via the `GDOT.time` syntax. Access the “time” columns from the other tables via similar syntax.\n",
    "\n",
    "    - Using the SQL query, read the data from the database into a Pandas DataFrame. \n",
    "    \n",
    "    - Finally, we review the resulting DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Here we create a SQL query to join each table in the portfolio into a single DataFrame \n",
    "# We use the time column from each table as the basis for the join\n",
    "query = \"\"\"\n",
    "SELECT  *\n",
    "FROM GDOT, GS, PYPL, SQ\n",
    "WHERE GDOT.time = GS.time\n",
    "AND   PYPL.time = SQ.time\n",
    "AND   GDOT.time = PYPL.time\n",
    "\"\"\"\n",
    "#['GDOT', 'GS', 'PYPL', 'SQ']\n",
    "# Using the query, read the data from the database into a Pandas DataFrame\n",
    "frm='%Y%m%d %H:%M:%S'\n",
    "etf_portfolio = pd.read_sql_query(query, engine, parse_dates={'time':frm})\n",
    "\n",
    "# Review the resulting DataFrame\n",
    "print('\\n')\n",
    "print(\"\\033[1m Join Tables from ['GDOT', 'GS', 'PYPL', 'SQ'] on dates\")\n",
    "display(etf_portfolio.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create a DataFrame that averages the “daily_returns” columns for all four assets. Review the resulting DataFrame.\n",
    "\n",
    " We assume that this ETF contains equally weighted returns, and average the daily returns for all assets to get the average returns of the portfolio. We use the average returns of the portfolio to calculate the annualized returns and the cumulative returns. For the calculation to get the average daily returns for the portfolio, we use the following code:\n",
    "\n",
    " ```python\n",
    " etf_portfolio_returns = etf_portfolio['daily_returns'].mean(axis=1)\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame that averages the “daily_returns” columns for all four assets. Review the resulting DataFrame.\n",
    "etf_portfolio_returns = etf_portfolio['daily_returns'].mean(axis=1)\n",
    "print('\\033[1mETF Portfolio Returns (%)')\n",
    "display(round((etf_portfolio_returns*100),2).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# As a second view, we create a DataFrame that displays the value of the “daily_returns” for all four assets only, and assign an index.\n",
    "# Use the time column from each table as the basis for the join\n",
    "query2 = \"\"\"\n",
    "SELECT  GDOT.time, GDOT.daily_returns as 'GDOT.daily_returns',\n",
    "     GS.daily_returns as 'GS.daily_returns',\n",
    "   PYPL.daily_returns as 'PYPL.daily_returns',\n",
    "     SQ.daily_returns as 'SQ.daily_returns'\n",
    "FROM GDOT, GS, PYPL, SQ\n",
    "WHERE GDOT.time = GS.time\n",
    "AND   PYPL.time = SQ.time\n",
    "AND   GDOT.time = PYPL.time\n",
    "\"\"\"\n",
    "\n",
    "#['GDOT', 'GS', 'PYPL', 'SQ']\n",
    "# Using the query, read the data from the database into a Pandas DataFrame\n",
    "fmt='%Y%m%d %H:%M:%S'\n",
    "etf_portfolio2 = pd.read_sql_query(\n",
    "        query2\n",
    "        , engine\n",
    "        , parse_dates={'time':fmt}\n",
    ")\n",
    "etf_portfolio2 = etf_portfolio2.set_index(\"time\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('\\n')\n",
    "print('\\033[1m                         Daily individual returns in time (%)')\n",
    "display(round((etf_portfolio2 * 100),2))\n",
    "\n",
    "\n",
    "# We repeat the calculation of average daily returns using this table\n",
    "etf_portfolio_returns = etf_portfolio2.mean( axis=1 )\n",
    "\n",
    "# Review the resulting DataFrame\n",
    "display(\"ETF Returns (%)\")\n",
    "display(round(etf_portfolio_returns*100,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Use the average daily returns in the etf_portfolio_returns DataFrame to calculate the annualized returns for the portfolio. Display the annualized return value of the ETF portfolio.\n",
    "\n",
    "To calculate the expected annualized returns, we multiply the mean of the `etf_portfolio_returns` values by 252.\n",
    "\n",
    "To convert the decimal values to percentages, we multiply the results by 100 before printing or plotting the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the average daily returns in the etf_portfolio_returns DataFrame \n",
    "# to calculate the annualized return for the portfolio. \n",
    "annualized_etf_portfolio_returns = etf_portfolio_returns.mean()*252\n",
    "\n",
    "print(f\"The expected annualized return, calculated using daily average return in the period, times 252 trading days is: {annualized_etf_portfolio_returns*100:,.2f}% \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: We use the average daily returns in the `etf_portfolio_returns` DataFrame to calculate the cumulative returns of the ETF portfolio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the average daily returns provided by the etf_portfolio_returns DataFrame \n",
    "# to calculate the cumulative returns\n",
    "# This is the growth of 1[USD] initial investment\n",
    "etf_cumulative_returns = (1 + etf_portfolio_returns).cumprod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROI\n",
    "etf_cumulative_return_above_initial_investment = etf_cumulative_returns[len(etf_cumulative_returns) - 1] - 1\n",
    "growth_of_1usd_initial_investment = etf_cumulative_return_above_initial_investment + 1\n",
    "\n",
    "# Display the final cumulative return value\n",
    "print(f\"The cumulative return of the investment in the full period, above the initial investment (no-annualized) is of {etf_cumulative_return_above_initial_investment*100:,.2f}%\")\n",
    "print(f\"The growth of $1.00 initial investment in the full period is ${growth_of_1usd_initial_investment:.2f} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusting columns names for proper graph variables\n",
    "etf_cumulative_returns_df = pd.DataFrame(etf_cumulative_returns, columns=['Growth of 1[USD] Initial Investment'])\n",
    "\n",
    "etf_cumulative_returns_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Using hvPlot, we create an interactive line plot that visualizes the cumulative return values of the ETF portfolio. Reflect the “time” column of the DataFrame on the x-axis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using hvplot, create an interactive line plot that visualizes the ETF portfolios cumulative return values.\n",
    "etf_cumulative_returns_df.hvplot(\n",
    "    title=\"ETF - Equally Weighted FinTech Stocks (GDOT, GS, PYPL, SQ) Growth of 1 USD Initial Investment -- Dec-16-2016 to Dec 4th 2020\"\n",
    "    , ylabel=\"Cumulative Investment [$]\"\n",
    "    , xlabel= \"Date\"\n",
    "    , width=900\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Finally we will save the results in a new table in a new database, to keep the original etf.db file intact for future use of the script. This database will just contain just ETF returns. In this case, we will include the equally weighted column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will save the results on a new database called etf_returns that will contain only full etf returns, no components.\n",
    "# Create a temporary SQLite database and save it as etf_returns.db\n",
    "database_connection_string2 = 'sqlite:///etf_returns.db'\n",
    "\n",
    "# Create an engine to interact with the SQLite database\n",
    "engine2 = sqlalchemy.create_engine(database_connection_string2)\n",
    "inspect(engine2).get_table_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We prepare column names for the data\n",
    "etf_portfolio_returns_df = etf_portfolio_returns.to_frame()\n",
    "etf_portfolio_returns_df.columns =['Equally_weighted']\n",
    "etf_portfolio_returns_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We insert in the new table the ETF equally weighted returns just calculated\n",
    "etf_portfolio_returns_df.to_sql('ETF_returns', engine2, index=True, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We inspect the column names in the table to confirm is there \n",
    "columns_table = inspect(engine2).get_columns('ETF_returns') #schema is optional\n",
    "\n",
    "# We print them with the datatype\n",
    "for c in columns_table :\n",
    "   print(c['name'], c['type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We check the data have been saved in etf_returns.db\n",
    "read_data_query = \"\"\"\n",
    "    SELECT * from ETF_returns\n",
    "\"\"\"\n",
    "pd.read_sql_query(read_data_query, con=engine2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deployment of the Notebook as a Web Application\n",
    "\n",
    "For this part, we completed the following steps:\n",
    "\n",
    "1. Use the Voilà library to deploy the notebook as a web application locally on the computer.\n",
    "\n",
    "2. Include a screen recording in the GitHub repository, as well as screenshots in the \"README.md\" file to show how the web application appears when using Voilà."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To include in future developments:\n",
    "to use the INSERT statetment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dev)",
   "language": "python",
   "name": "dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
